{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c3af13d",
   "metadata": {},
   "source": [
    "# Artificial data generation\n",
    "\n",
    "In this notebook, we show how to generate the artificial data sets used to train and test our models. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ac209",
   "metadata": {},
   "source": [
    "## Asia model\n",
    "\n",
    "We load in the Asia example model from the [BNlearn](http://bnlearn.com/bnrepository/discrete-small.html#asia) repository. We remove the either node and adapt the conditional probability tables accordingly, so the joint probability distribution remains the same. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ce5528",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.utils import get_example_model\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "\n",
    "def get_asia_model(): \n",
    "\n",
    "    \"\"\"\n",
    "    create the ground-truth asia model based on http://bnlearn.com/bnrepository/discrete-small.html#asia\n",
    "    remove the \"either\" node and adapt CPTs accordingly\n",
    "    \"\"\"\n",
    "\n",
    "    asia = get_example_model('asia')\n",
    "\n",
    "    asia.remove_node(\"either\")\n",
    "    asia.add_edge(u=\"tub\",v=\"dysp\")\n",
    "    asia.add_edge(u=\"tub\",v=\"xray\")\n",
    "    asia.add_edge(u=\"lung\",v=\"dysp\")\n",
    "    asia.add_edge(u=\"lung\",v=\"xray\")\n",
    "\n",
    "    cpd_xray = TabularCPD(variable='xray', variable_card=2, \n",
    "                          values=[[0.98, 0.98, 0.98, 0.05], # xray \"yes\": [L=y,T=y],[L=y,T=n],[L=n,T=y],[L=n,T=n]\n",
    "                                  [0.02, 0.02, 0.02, 0.95]], # xray \"no\": [id.]\n",
    "                          evidence=['lung', 'tub'],\n",
    "                          evidence_card=[2, 2],\n",
    "                          state_names={'xray': ['yes', 'no'],\n",
    "                                       'lung': ['yes', 'no'],\n",
    "                                       'tub': ['yes', 'no']})\n",
    "\n",
    "    asia.add_cpds(cpd_xray)\n",
    "\n",
    "    cpd_dysp = TabularCPD(variable='dysp', variable_card=2, \n",
    "                          values=[[0.9, 0.9, 0.9, 0.8, 0.7, 0.7, 0.7, 0.1], # xray \"yes\": [B=y,L=y,T=y],[B=y,L=y,T=n],[B=y,L=n,T=y],[B=y,L=n,T=n],[B=n,L=y,T=y],[B=n,L=y,T=n],[B=n,L=n,T=y],[B=n,L=n,T=n]\n",
    "                                  [0.1, 0.1, 0.1, 0.2, 0.3, 0.3, 0.3, 0.9]], # xray \"no\": [id.]\n",
    "                          evidence=['bronc', 'lung', 'tub'],\n",
    "                          evidence_card=[2, 2, 2],\n",
    "                          state_names={'dysp': ['yes', 'no'],\n",
    "                                       'bronc': ['yes', 'no'],\n",
    "                                       'lung': ['yes', 'no'],\n",
    "                                       'tub': ['yes', 'no']})\n",
    "\n",
    "    asia.add_cpds(cpd_dysp)\n",
    "    \n",
    "    return asia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8f6f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "GT_model = get_asia_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b637191",
   "metadata": {},
   "source": [
    "## Train set\n",
    "\n",
    "We obtain 20000 samples from the ground-truth joint distribution as a full training set (from which we can subsample to get training sets of smaller sizes). The training set is sampled in chunks of 1000 samples, since our experiments showed that the BayesianModelSampling method in the pgmpy library could get unstable if we extract a large (e.g. 20000) number of samples at once. \n",
    "\n",
    "The resulting train set is written to a file, where the first line indicates the variable that is represented by each position in the sample vector, and the second line indicates the possible classes per variable. The observed class per variable in a sample is indicated by a one-hot encoding, with the positions corresponding to the variable and classes as given by the first two lines of the file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4f28b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.sampling import BayesianModelSampling\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "def sample_dataset(model, N_samples, c_size, seed, file_name):\n",
    "    \n",
    "    # extract samples in chunks of size c_size \n",
    "    random.seed(seed)\n",
    "    seed = random.randint(1, 1000)\n",
    "    inference = BayesianModelSampling(model)\n",
    "    samples = inference.forward_sample(size=c_size, seed=seed, show_progress=False)\n",
    "    N_chunks = N_samples//c_size\n",
    "    for _ in range(N_chunks-1):\n",
    "        seed = random.randint(1, 1000)\n",
    "        samp = inference.forward_sample(size=c_size, seed=seed, show_progress=False)\n",
    "        samples = pd.concat([samples, samp], ignore_index=True)\n",
    "    \n",
    "    # create dataframe with one-hot encoding of samples\n",
    "    states = model.states\n",
    "    for key in states:\n",
    "        for value in states[key]:\n",
    "            samples[key + \" \" + value] = samples[key] == value\n",
    "    for key in states: \n",
    "        del samples[key]\n",
    "    samples = samples.replace(to_replace={True: 1.0, False: 0.0})\n",
    "    \n",
    "    # write resulting samples to file\n",
    "    with open(file_name, 'w') as outfile: \n",
    "        var_names = [col_name.split(\" \")[0] for col_name in samples.columns]\n",
    "        state_names = [col_name.split(\" \")[1] for col_name in samples.columns]\n",
    "        outfile.write(','.join(var_names)+'\\n')\n",
    "        outfile.write(','.join(state_names)+'\\n')\n",
    "        for index, row in samples.iterrows(): \n",
    "            r = [str(e) for e in row]\n",
    "            outfile.write(','.join(list(r))+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b64384ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset(GT_model, 20000, 1000, 2022, \"data/asia/train_samples.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e48c995e",
   "metadata": {},
   "source": [
    "## Test set: all queries\n",
    "\n",
    "We create a test set to compute the total MAE. This means we generate all possible evidence combinations with all possible values, and obtain the conditional probability for each query from the ground-truth model. \n",
    "\n",
    "We write the result to a file, with each line representing one query. Again, the first two lines of the file contain the ordering of the variables and their states. Evidence settings are represented with integer one-hot encoding (either 1 or 0 for each of the states for a particular variable), while expected target probabilities are represented by a float (the targets belonging to the same variable all sum to one). When receiving such a file, the *TestQueryDataset* class is able to extract the evidence/target mask, the input vector (values of the evidence variables, targets are nan) and the target probability vector (evidence variables are nan). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6c15c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pgmpy.inference import VariableElimination\n",
    "import itertools\n",
    "import numpy as np\n",
    "\n",
    "def generate_all_queries(model):\n",
    "    \n",
    "    all_vars = list(model.nodes) # list of vars in the model\n",
    "    all_states = model.states # dictionary: key=var, value=list of state names\n",
    "    \n",
    "    one_hot = {var:{state:i for i,state in enumerate(all_states[var])} for var in all_states} # specifies the index within the one-hot vector\n",
    "                                                                                              # which is equal to 1 when a state is selected (per var)\n",
    "                                                                                              # key=var, value=dict of state:index\n",
    "    \n",
    "    infer = VariableElimination(model) # inference object\n",
    "    n = len(all_vars)\n",
    "    df = [] # dataset which will contain all samples\n",
    "    \n",
    "    for r in range(1, n): # loop over r = number of vars included in evidence state in this iteration\n",
    "        comb = itertools.combinations(all_vars, r) # get all sets of r vars (= evidence sets)\n",
    "        \n",
    "        for c in comb: \n",
    "            states = []\n",
    "            for var in c: \n",
    "                states.append(all_states[var])\n",
    "            prod = itertools.product(*states) # get all possible assignments of states to the set of vars in c\n",
    "            \n",
    "            for p in prod:\n",
    "                \n",
    "                evidence = {}\n",
    "                for i in range(len(p)): \n",
    "                    evidence[c[i]] = p[i] # create the evidence set\n",
    "                res = {}\n",
    "                for e in evidence: # add one-hot vector to sample, representing which state was selected per evidence var\n",
    "                    r = np.zeros(len(all_states[e]), dtype=int) \n",
    "                    r[one_hot[e][evidence[e]]] = 1\n",
    "                    res[e] = r\n",
    "                    \n",
    "                for var in all_vars: # query probability for all vars which are not included in evidence\n",
    "                    if var not in evidence: \n",
    "                        answer = infer.query([var], evidence=evidence, show_progress=False)\n",
    "                        res[var] = answer.values # add array of probs to sample\n",
    "                df.append(res) # add sample to dataset\n",
    "                \n",
    "    res = {}\n",
    "    \n",
    "    for var in all_vars:\n",
    "        res[var] = infer.query([var], show_progress=False).values # add no-evidence probabilities (marginalize out all other vars)\n",
    "    df.append(res)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d78b46b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_test_file(model, dataset, path):\n",
    "    with open(path, 'w') as outfile: \n",
    "        all_states = model.states # dictionary: key=var, value=list of state names\n",
    "        mapping_vars = ','.join([','.join(len(all_states[var])*[var]) for var in all_states])\n",
    "        mapping_labels = ','.join([','.join(all_states[var]) for var in all_states])\n",
    "        outfile.write(mapping_vars+'\\n')\n",
    "        outfile.write(mapping_labels+'\\n')\n",
    "        for r in dataset:\n",
    "            line = []\n",
    "            for var in all_states:\n",
    "                if r[var].dtype != int:\n",
    "                    line += [\"{:.5f}\".format(e) for e in r[var]]\n",
    "                else:\n",
    "                    line += [str(e) for e in r[var]]\n",
    "            outfile.write(','.join(line)+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91c5fe76",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_dataset = generate_all_queries(GT_model)\n",
    "write_test_file(GT_model, query_dataset, \"data/asia_new/total_test_set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28994950",
   "metadata": {},
   "source": [
    "## Test set: sampled queries\n",
    "\n",
    "We gather 1000 test samples from the ground-truth distribution. For each of these, we generate a random evidence/target mask. The selected evidence variables are set to the observed value in the sample, while the conditional target probabilities are obtained from the ground-truth model for the remaining variables. We write the results to another file, with the same structure as the *total_test_set* file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1ccf0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3902f593378545f98f22459213316b7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Nsamples = 1000\n",
    "inference = BayesianModelSampling(GT_model)\n",
    "test_samples = inference.forward_sample(size=Nsamples, seed=2022) # use a different seed than sampling training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "65330310",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_query(model, evidence, target):\n",
    "    \n",
    "    all_states = model.states\n",
    "    one_hot = {var:{state:i for i,state in enumerate(all_states[var])} for var in all_states}\n",
    "    \n",
    "    res = {}\n",
    "    \n",
    "    # add one-hot vector to sample representation, storing which state was selected per evidence var\n",
    "    for e in evidence: \n",
    "        r = np.zeros(len(all_states[e]), dtype=int) \n",
    "        r[one_hot[e][evidence[e]]] = 1\n",
    "        res[e] = r\n",
    "\n",
    "    # infer values of all targets for given evidence set \n",
    "    infer = VariableElimination(model)\n",
    "    for t in target:\n",
    "        pred = infer.query([t], evidence=evidence, show_progress=False)\n",
    "        res[t] = pred.values\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ee0490a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample_queries(model, test_samples, seed):\n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    all_states = model.states\n",
    "    one_hot = {var:{state:i for i,state in enumerate(all_states[var])} for var in all_states}\n",
    "    all_vars = list(all_states.keys())\n",
    "\n",
    "    df = []\n",
    "\n",
    "    for i, sample in test_samples.iterrows(): \n",
    "\n",
    "        # generate random mask\n",
    "        mask = np.random.randint(0, 2, size = len(all_vars))\n",
    "        while np.isclose(np.sum(mask), 0): # we will not be able to find zero-mask in ground-truth probability set \n",
    "            mask = np.random.randint(0, 2, size = len(all_vars))\n",
    "\n",
    "        # create evidence and target set\n",
    "        evidence = {}\n",
    "        target = []\n",
    "        for i in range(len(mask)):\n",
    "            if mask[i] == 1:\n",
    "                var_name = all_vars[i]\n",
    "                evidence[var_name] = sample[var_name]\n",
    "            else: \n",
    "                var_name = all_vars[i]\n",
    "                target.append(var_name)\n",
    "                \n",
    "        res = pred_query(model, evidence, target)\n",
    "\n",
    "        # add sample to dataset\n",
    "        df.append(res)\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4b3726aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dataset = generate_sample_queries(GT_model, test_samples, 2022)\n",
    "write_to_file(GT_model, sample_dataset, \"data/asia_new/sample_test_set.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db7c380",
   "metadata": {},
   "source": [
    "## Independence relations\n",
    "\n",
    "We can automatically extract all independence relations from the causal structure of the Asia model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d275ecea",
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = GT_model.get_independencies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "82b63d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/asia_new/independencies.txt\", 'w', encoding='utf-8') as file:\n",
    "    file.write(str(ind))\n",
    "    file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51036e20",
   "metadata": {},
   "source": [
    "## Robustness experiments\n",
    "\n",
    "We test how the performance of our models changes when the causal structure of the bayesian network is partially misspecified. To this end, we randomly select 5 edges to remove from, and 5 edges to add to the ground-truth structure, and we generate the independence relations from these new models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01de415d",
   "metadata": {},
   "source": [
    "### Remove edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ab757487",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54eafb3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['tub' 'xray']\n",
      " ['lung' 'xray']\n",
      " ['smoke' 'lung']\n",
      " ['tub' 'dysp']\n",
      " ['asia' 'tub']]\n"
     ]
    }
   ],
   "source": [
    "edges = np.array(GT_model.edges)\n",
    "np.random.seed(2022)\n",
    "edge_idx = np.random.choice(np.arange(len(edges)), size=5, replace=False)\n",
    "edge_rm = edges[edge_idx, :]\n",
    "print(edge_rm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019e2fb3",
   "metadata": {},
   "source": [
    "The 5 edges above will be removed one by one from the ground-truth structure to obtain a partially incorrect causal structure. From this new structure, the independence relations can be automatically extracted as before, and can be used to train our neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7f6b15bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ind_file_rm(GT_model, i, u, v):\n",
    "    model = GT_model.copy()\n",
    "    with open(\"data/asia/remove_edges/ind_r\"+str(i)+\".txt\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        model.remove_edge(u, v)\n",
    "        ind = model.get_independencies()\n",
    "        file.write(str(ind))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "337f9699",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_ind_file_rm(GT_model, 1, \"tub\", \"xray\")\n",
    "write_ind_file_rm(GT_model, 2, \"lung\", \"xray\")\n",
    "write_ind_file_rm(GT_model, 3, \"smoke\", \"lung\")\n",
    "write_ind_file_rm(GT_model, 4, \"tub\", \"dysp\")\n",
    "write_ind_file_rm(GT_model, 5, \"asia\", \"tub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46be30",
   "metadata": {},
   "source": [
    "### Add edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8104b3c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['asia' 'tub' 'smoke' 'lung' 'bronc' 'xray' 'dysp']\n"
     ]
    }
   ],
   "source": [
    "nodes = np.array(GT_model.nodes)\n",
    "print(nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f341fc",
   "metadata": {},
   "source": [
    "We need to randomly select 5 pairs of nodes (= a random edge) from the list above, keeping the following conditions in mind: \n",
    "1. the edge is not present in the ground-truth DAG\n",
    "2. the inverted edge is not present in the ground-truth DAG\n",
    "3. adding the edge does not introduce any cycles in the DAG\n",
    "\n",
    "We print the first 20 randomly chosen edges and manually select the first 5 that adhere to the criteria above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0ee04f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['smoke' 'dysp']\n",
      "['lung' 'xray']\n",
      "['dysp' 'asia']\n",
      "['tub' 'bronc']\n",
      "['bronc' 'tub']\n",
      "['lung' 'bronc']\n",
      "['dysp' 'lung']\n",
      "['dysp' 'asia']\n",
      "['lung' 'xray']\n",
      "['tub' 'bronc']\n",
      "['lung' 'dysp']\n",
      "['lung' 'smoke']\n",
      "['asia' 'dysp']\n",
      "['asia' 'xray']\n",
      "['lung' 'asia']\n",
      "['tub' 'smoke']\n",
      "['bronc' 'xray']\n",
      "['dysp' 'bronc']\n",
      "['smoke' 'tub']\n",
      "['asia' 'dysp']\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(2022)\n",
    "for i in range(20):\n",
    "    node_names = np.random.choice(nodes, size = 2, replace=False)\n",
    "    print(node_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001f1ee3",
   "metadata": {},
   "source": [
    "The selected edges are the following: \n",
    "1. smoke -> dysp\n",
    "2. tub -> bronc\n",
    "3. lung -> bronc\n",
    "4. asia -> dysp\n",
    "5. asia -> xray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f55cca38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_ind_file_add(GT_model, i, u, v):\n",
    "    model = GT_model.copy()\n",
    "    with open(\"data/asia/add_edges/ind_a\"+str(i)+\".txt\", mode=\"w\", encoding=\"utf-8\") as file:\n",
    "        model.add_edge(u, v)\n",
    "        ind = model.get_independencies()\n",
    "        file.write(str(ind))\n",
    "        file.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "80aabe00",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_ind_file_add(GT_model, 1, \"smoke\", \"dysp\")\n",
    "write_ind_file_add(GT_model, 2, \"tub\", \"bronc\")\n",
    "write_ind_file_add(GT_model, 3, \"lung\", \"bronc\")\n",
    "write_ind_file_add(GT_model, 4, \"asia\", \"dysp\")\n",
    "write_ind_file_add(GT_model, 5, \"asia\", \"xray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089e4c62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
